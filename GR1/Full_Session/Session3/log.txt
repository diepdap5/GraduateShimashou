/home/diepdn/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/diepdn/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/diepdn/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/diepdn/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/diepdn/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/diepdn/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/diepdn/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/diepdn/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/diepdn/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/diepdn/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/diepdn/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/diepdn/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From mlp.py:46: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

2020-06-30 12:11:35.799563: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-06-30 12:11:35.822094: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2020-06-30 12:11:35.822407: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2a815c0 executing computations on platform Host. Devices:
2020-06-30 12:11:35.822437: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-06-30 12:11:58.684874: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
step: 1, loss: 0.0020715268328785896
step: 2, loss: 1.1920925402364446e-07
step: 3, loss: 0.0
step: 4, loss: 0.0
step: 5, loss: 0.0
step: 6, loss: 0.0
step: 7, loss: 0.0
step: 8, loss: 0.0
step: 9, loss: 0.0
step: 10, loss: 14.740242958068848
step: 11, loss: 32.6609001159668
step: 12, loss: 27.336549758911133
step: 13, loss: 20.885658264160156
step: 14, loss: 13.033677101135254
step: 15, loss: 5.088021755218506
step: 16, loss: 0.8165655732154846
step: 17, loss: 0.01796560361981392
step: 18, loss: 0.0001822131162043661
step: 19, loss: 3.5285825106257107e-07
step: 20, loss: 1.192092646817855e-08
step: 21, loss: 0.0
step: 22, loss: 23.624563217163086
step: 23, loss: 31.79952621459961
step: 24, loss: 29.7972412109375
step: 25, loss: 26.408302307128906
step: 26, loss: 22.548559188842773
step: 27, loss: 17.576824188232422
step: 28, loss: 13.57876968383789
step: 29, loss: 8.493163108825684
step: 30, loss: 5.079425811767578
step: 31, loss: 1.779649019241333
step: 32, loss: 0.1363280713558197
step: 33, loss: 0.03304716944694519
step: 34, loss: 17.136913299560547
step: 35, loss: 18.733844757080078
step: 36, loss: 17.563875198364258
step: 37, loss: 16.33783721923828
step: 38, loss: 14.167057991027832
step: 39, loss: 11.994135856628418
step: 40, loss: 9.640185356140137
step: 41, loss: 7.390129566192627
step: 42, loss: 5.370547294616699
step: 43, loss: 3.8952949047088623
step: 44, loss: 1.5208566188812256
step: 45, loss: 0.42547088861465454
step: 46, loss: 0.0
step: 47, loss: 0.0
step: 48, loss: 0.0
step: 49, loss: 0.0
step: 50, loss: 0.0
step: 51, loss: 0.0
step: 52, loss: 0.0
step: 53, loss: 0.0
step: 54, loss: 0.0
step: 55, loss: 0.0
step: 56, loss: 0.0
step: 57, loss: 0.0
step: 58, loss: 0.0
step: 59, loss: 0.0
step: 60, loss: 0.0
step: 61, loss: 0.0
step: 62, loss: 0.0
step: 63, loss: 0.0
step: 64, loss: 0.0
step: 65, loss: 0.0
step: 66, loss: 0.0
step: 67, loss: 0.0
step: 68, loss: 0.0
step: 69, loss: 0.0
step: 70, loss: 0.0
step: 71, loss: 0.0
step: 72, loss: 0.0
step: 73, loss: 0.0
step: 74, loss: 0.0
step: 75, loss: 0.0
step: 76, loss: 0.0
step: 77, loss: 0.0
step: 78, loss: 0.0
step: 79, loss: 0.0
step: 80, loss: 0.0
step: 81, loss: 0.0
step: 82, loss: 0.0
step: 83, loss: 0.0
step: 84, loss: 0.0
step: 85, loss: 0.0
step: 86, loss: 0.0
step: 87, loss: 0.0
step: 88, loss: 0.0
step: 89, loss: 0.0
step: 90, loss: 0.0
step: 91, loss: 0.0
step: 92, loss: 0.0
step: 93, loss: 0.0
step: 94, loss: 0.0
step: 95, loss: 0.0
step: 96, loss: 0.0
step: 97, loss: 0.0
step: 98, loss: 0.0
step: 99, loss: 0.0
step: 100, loss: 0.0
step: 101, loss: 0.0
step: 102, loss: 0.0
step: 103, loss: 0.0
step: 104, loss: 0.0
step: 105, loss: 0.0
step: 106, loss: 0.0
step: 107, loss: 0.0
step: 108, loss: 0.0
step: 109, loss: 0.0
step: 110, loss: 0.0
step: 111, loss: 0.0
step: 112, loss: 0.0
step: 113, loss: 0.0
step: 114, loss: 0.0
step: 115, loss: 0.0
step: 116, loss: 0.0
step: 117, loss: 0.0
step: 118, loss: 0.0
step: 119, loss: 0.0
step: 120, loss: 0.0
step: 121, loss: 0.0
step: 122, loss: 0.0
step: 123, loss: 0.0
step: 124, loss: 0.0
step: 125, loss: 0.0
step: 126, loss: 0.0
step: 127, loss: 0.0
step: 128, loss: 0.0
step: 129, loss: 0.0
step: 130, loss: 0.0
step: 131, loss: 0.0
step: 132, loss: 0.0
step: 133, loss: 0.0
step: 134, loss: 0.0
step: 135, loss: 0.0
step: 136, loss: 0.0
step: 137, loss: 0.0
step: 138, loss: 0.0
step: 139, loss: 0.0
step: 140, loss: 0.0
step: 141, loss: 0.0
step: 142, loss: 0.0
step: 143, loss: 0.0
step: 144, loss: 0.0
step: 145, loss: 0.0
step: 146, loss: 0.0
step: 147, loss: 0.0
step: 148, loss: 0.0
step: 149, loss: 0.0
step: 150, loss: 0.0
step: 151, loss: 0.0
step: 152, loss: 0.0
step: 153, loss: 0.0
step: 154, loss: 0.0
step: 155, loss: 0.0
step: 156, loss: 0.0
step: 157, loss: 0.0
step: 158, loss: 0.0
step: 159, loss: 0.0
step: 160, loss: 0.0
step: 161, loss: 0.0
step: 162, loss: 0.0
step: 163, loss: 0.0
step: 164, loss: 0.0
step: 165, loss: 0.0
step: 166, loss: 0.0
step: 167, loss: 0.0
step: 168, loss: 0.0
step: 169, loss: 0.0
step: 170, loss: 0.0
step: 171, loss: 0.0
step: 172, loss: 0.0
step: 173, loss: 0.0
step: 174, loss: 0.0
step: 175, loss: 0.0
step: 176, loss: 0.0
step: 177, loss: 0.0
step: 178, loss: 0.0
step: 179, loss: 0.0
step: 180, loss: 0.0
step: 181, loss: 0.0
step: 182, loss: 0.0
step: 183, loss: 0.0
step: 184, loss: 0.0
step: 185, loss: 0.0
step: 186, loss: 0.0
step: 187, loss: 0.0
step: 188, loss: 0.0
step: 189, loss: 0.0
step: 190, loss: 0.0
step: 191, loss: 0.0
step: 192, loss: 0.0
step: 193, loss: 0.0
step: 194, loss: 0.0
step: 195, loss: 0.0
step: 196, loss: 0.0
step: 197, loss: 0.0
step: 198, loss: 0.0
step: 199, loss: 0.0
step: 200, loss: 0.0
step: 201, loss: 0.0
step: 202, loss: 0.0
step: 203, loss: 0.0
step: 204, loss: 0.0
step: 205, loss: 0.0
step: 206, loss: 0.0
step: 207, loss: 0.0
step: 208, loss: 0.0
step: 209, loss: 0.0
step: 210, loss: 0.0
step: 211, loss: 0.0
step: 212, loss: 0.0
step: 213, loss: 0.0
step: 214, loss: 0.0
step: 215, loss: 0.0
step: 216, loss: 0.0
step: 217, loss: 0.0
step: 218, loss: 0.0
step: 219, loss: 0.0
step: 220, loss: 0.0
step: 221, loss: 0.0
step: 222, loss: 0.0
step: 223, loss: 0.0
step: 224, loss: 0.0
step: 225, loss: 0.0
step: 226, loss: 0.2902742624282837
step: 227, loss: 0.41759222745895386
step: 228, loss: 0.9130215644836426
step: 229, loss: 0.6877540349960327
step: 230, loss: 0.5916142463684082
step: 231, loss: 0.363240510225296
step: 232, loss: 0.3761293292045593
step: 233, loss: 0.6569381952285767
step: 234, loss: 0.44046977162361145
step: 235, loss: 0.310098260641098
step: 236, loss: 0.47025352716445923
step: 237, loss: 0.267368882894516
step: 238, loss: 0.25389647483825684
step: 239, loss: 0.42037639021873474
step: 240, loss: 0.26671943068504333
step: 241, loss: 0.22849006950855255
step: 242, loss: 0.32540610432624817
step: 243, loss: 0.2182832956314087
step: 244, loss: 0.5043222904205322
step: 245, loss: 0.2005198448896408
step: 246, loss: 0.36331790685653687
step: 247, loss: 0.3242884874343872
step: 248, loss: 0.1261548548936844
step: 249, loss: 0.3861064910888672
step: 250, loss: 0.2253490835428238
step: 251, loss: 0.28454113006591797
step: 252, loss: 0.10038957744836807
step: 253, loss: 0.2226923108100891
step: 254, loss: 0.2547702491283417
step: 255, loss: 0.385407418012619
step: 256, loss: 0.3684829771518707
step: 257, loss: 0.2032744586467743
step: 258, loss: 0.2800911068916321
step: 259, loss: 0.2958897352218628
step: 260, loss: 0.14872342348098755
step: 261, loss: 0.38788580894470215
step: 262, loss: 0.271514356136322
step: 263, loss: 0.1920195072889328
step: 264, loss: 0.2947542667388916
step: 265, loss: 0.4727493226528168
step: 266, loss: 0.1991444081068039
step: 267, loss: 0.3892174959182739
step: 268, loss: 0.501457154750824
step: 269, loss: 0.17804184556007385
step: 270, loss: 0.304726779460907
step: 271, loss: 0.2206389456987381
step: 272, loss: 0.3087679445743561
step: 273, loss: 0.3182218074798584
step: 274, loss: 0.21032701432704926
step: 275, loss: 0.222794309258461
step: 276, loss: 0.16116400063037872
step: 277, loss: 0.16502346098423004
step: 278, loss: 0.30104783177375793
step: 279, loss: 0.39056289196014404
step: 280, loss: 0.32957160472869873
step: 281, loss: 0.26622435450553894
step: 282, loss: 0.18942385911941528
step: 283, loss: 0.32656413316726685
step: 284, loss: 0.23736564815044403
step: 285, loss: 0.2292204052209854
step: 286, loss: 0.1501268893480301
step: 287, loss: 0.2403268963098526
step: 288, loss: 0.3177291750907898
step: 289, loss: 0.2961067259311676
step: 290, loss: 0.30410850048065186
step: 291, loss: 0.2799953520298004
step: 292, loss: 0.33472147583961487
step: 293, loss: 0.24967963993549347
step: 294, loss: 0.288211464881897
step: 295, loss: 0.3954221308231354
step: 296, loss: 0.2553102672100067
step: 297, loss: 0.30176031589508057
step: 298, loss: 0.2744619846343994
step: 299, loss: 0.27516257762908936
step: 300, loss: 0.3373035192489624
step: 301, loss: 0.21619579195976257
step: 302, loss: 0.24972277879714966
step: 303, loss: 0.20199403166770935
step: 304, loss: 0.3319840729236603
step: 305, loss: 0.30286967754364014
step: 306, loss: 0.24102281033992767
step: 307, loss: 0.18943138420581818
step: 308, loss: 0.49474167823791504
step: 309, loss: 0.34634941816329956
step: 310, loss: 0.293119341135025
step: 311, loss: 0.44340819120407104
step: 312, loss: 0.20025230944156647
step: 313, loss: 0.32262295484542847
step: 314, loss: 0.21644093096256256
step: 315, loss: 0.24401141703128815
step: 316, loss: 0.211877703666687
step: 317, loss: 0.32145270705223083
step: 318, loss: 0.30454203486442566
step: 319, loss: 0.41761770844459534
step: 320, loss: 0.21579250693321228
step: 321, loss: 0.2096157819032669
step: 322, loss: 0.24325835704803467
step: 323, loss: 0.39102500677108765
step: 324, loss: 0.24310867488384247
step: 325, loss: 0.31781548261642456
step: 326, loss: 0.16770513355731964
step: 327, loss: 0.23976868391036987
step: 328, loss: 0.3396379053592682
step: 329, loss: 0.19638456404209137
step: 330, loss: 0.2626340985298157
step: 331, loss: 0.11920973658561707
step: 332, loss: 0.14341381192207336
step: 333, loss: 0.2551540732383728
step: 334, loss: 0.06654810160398483
step: 335, loss: 0.17675265669822693
step: 336, loss: 0.33827564120292664
step: 337, loss: 0.2607060372829437
step: 338, loss: 0.13737840950489044
step: 339, loss: 0.15951359272003174
step: 340, loss: 0.20010723173618317
step: 341, loss: 0.19287212193012238
step: 342, loss: 0.301796019077301
step: 343, loss: 0.061532530933618546
step: 344, loss: 0.20220088958740234
step: 345, loss: 0.2798210084438324
step: 346, loss: 0.09269972145557404
step: 347, loss: 0.20373734831809998
step: 348, loss: 0.18549777567386627
step: 349, loss: 0.17158441245555878
step: 350, loss: 0.24552574753761292
step: 351, loss: 0.2692343592643738
step: 352, loss: 0.16715270280838013
step: 353, loss: 0.1960466355085373
step: 354, loss: 0.22424553334712982
step: 355, loss: 0.19000378251075745
step: 356, loss: 0.3580814301967621
step: 357, loss: 0.14144381880760193
step: 358, loss: 0.2378319352865219
step: 359, loss: 0.27539512515068054
step: 360, loss: 0.16338196396827698
step: 361, loss: 0.1532222032546997
step: 362, loss: 0.18774709105491638
step: 363, loss: 0.18344718217849731
step: 364, loss: 0.13715636730194092
step: 365, loss: 0.17082475125789642
step: 366, loss: 0.2388971745967865
step: 367, loss: 0.12226619571447372
step: 368, loss: 0.19489187002182007
step: 369, loss: 0.11124828457832336
step: 370, loss: 0.19866064190864563
step: 371, loss: 0.12525522708892822
step: 372, loss: 0.18827995657920837
step: 373, loss: 0.15425187349319458
step: 374, loss: 0.22412388026714325
step: 375, loss: 0.10582912713289261
step: 376, loss: 0.12777431309223175
step: 377, loss: 0.1590585708618164
step: 378, loss: 0.15828412771224976
step: 379, loss: 0.08447925746440887
step: 380, loss: 0.2563581168651581
step: 381, loss: 0.23275679349899292
step: 382, loss: 0.10637831687927246
step: 383, loss: 0.11131118983030319
step: 384, loss: 0.2634398639202118
step: 385, loss: 0.15250083804130554
step: 386, loss: 0.2818494737148285
step: 387, loss: 0.3913213610649109
step: 388, loss: 0.1822611689567566
step: 389, loss: 0.14034953713417053
step: 390, loss: 0.15848802030086517
step: 391, loss: 0.12195239961147308
step: 392, loss: 0.1615874320268631
step: 393, loss: 0.16871877014636993
step: 394, loss: 0.062131401151418686
step: 395, loss: 0.2445949763059616
step: 396, loss: 0.1637030392885208
step: 397, loss: 0.21367761492729187
step: 398, loss: 0.11814083904027939
step: 399, loss: 0.25598040223121643
step: 400, loss: 0.300650417804718
step: 401, loss: 0.19210414588451385
step: 402, loss: 0.28401848673820496
step: 403, loss: 0.15917588770389557
step: 404, loss: 0.24967242777347565
step: 405, loss: 0.13707317411899567
step: 406, loss: 0.2246319055557251
step: 407, loss: 0.0805477648973465
step: 408, loss: 0.22082601487636566
step: 409, loss: 0.11470391601324081
step: 410, loss: 0.18470244109630585
step: 411, loss: 0.12065620720386505
step: 412, loss: 0.14018717408180237
step: 413, loss: 0.09982097893953323
step: 414, loss: 0.0802534893155098
step: 415, loss: 0.13198445737361908
step: 416, loss: 0.13500475883483887
step: 417, loss: 0.1251232773065567
step: 418, loss: 0.10563871264457703
step: 419, loss: 0.06757474690675735
step: 420, loss: 0.13324770331382751
step: 421, loss: 0.1269557774066925
step: 422, loss: 0.061343904584646225
step: 423, loss: 0.07705308496952057
step: 424, loss: 0.08953274041414261
step: 425, loss: 0.07614630460739136
step: 426, loss: 0.09052985906600952
step: 427, loss: 0.05121195688843727
step: 428, loss: 0.08559509366750717
step: 429, loss: 0.05136718228459358
step: 430, loss: 0.2744482159614563
step: 431, loss: 0.1568828821182251
step: 432, loss: 0.17310865223407745
step: 433, loss: 0.20039056241512299
step: 434, loss: 0.19592632353305817
step: 435, loss: 0.12572774291038513
step: 436, loss: 0.22068962454795837
step: 437, loss: 0.13378891348838806
step: 438, loss: 0.2204045057296753
step: 439, loss: 0.35654541850090027
step: 440, loss: 0.19529643654823303
step: 441, loss: 0.22705261409282684
step: 442, loss: 0.10973026603460312
step: 443, loss: 0.1893402338027954
step: 444, loss: 0.17982204258441925
step: 445, loss: 0.2960643470287323
step: 446, loss: 0.10415180027484894
step: 447, loss: 0.1844220757484436
step: 448, loss: 0.055674005299806595
step: 449, loss: 0.14861540496349335
step: 450, loss: 0.22773505747318268
step: 451, loss: 0.19551953673362732
step: 452, loss: 0.11005672812461853
step: 453, loss: 0.17159909009933472
step: 454, loss: 0.12001489847898483
step: 455, loss: 0.09479854255914688
step: 456, loss: 0.0843096375465393
step: 457, loss: 0.11471778899431229
step: 458, loss: 0.16078385710716248
step: 459, loss: 0.09308724105358124
step: 460, loss: 0.08155465126037598
step: 461, loss: 0.10032419860363007
step: 462, loss: 0.08204895257949829
step: 463, loss: 0.1169033795595169
step: 464, loss: 0.11185405403375626
step: 465, loss: 0.05255943164229393
step: 466, loss: 0.10583540052175522
step: 467, loss: 0.121591717004776
step: 468, loss: 0.032787248492240906
step: 469, loss: 0.09238995611667633
step: 470, loss: 0.09036970138549805
step: 471, loss: 0.17695555090904236
step: 472, loss: 0.05265948176383972
step: 473, loss: 0.1126905158162117
step: 474, loss: 0.08496042340993881
step: 475, loss: 0.0495445616543293
step: 476, loss: 0.23156672716140747
step: 477, loss: 0.04895702376961708
step: 478, loss: 0.19953447580337524
step: 479, loss: 0.08392909169197083
step: 480, loss: 0.1648581176996231
step: 481, loss: 0.06452088803052902
step: 482, loss: 0.13210567831993103
step: 483, loss: 0.09224364906549454
step: 484, loss: 0.15809111297130585
step: 485, loss: 0.04864485189318657
step: 486, loss: 0.0703037828207016
step: 487, loss: 0.07513980567455292
step: 488, loss: 0.05092187970876694
step: 489, loss: 0.04260271042585373
step: 490, loss: 0.05565035715699196
step: 491, loss: 0.03177861124277115
step: 492, loss: 0.07388703525066376
step: 493, loss: 0.03659369796514511
step: 494, loss: 0.10905221849679947
step: 495, loss: 0.04371683672070503
step: 496, loss: 0.009877803735435009
step: 497, loss: 0.04491182416677475
step: 498, loss: 0.015358886681497097
step: 499, loss: 0.1180080771446228
step: 500, loss: 0.10218379646539688
step: 501, loss: 0.03435596451163292
step: 502, loss: 0.02987133525311947
step: 503, loss: 0.08842650055885315
step: 504, loss: 0.12902839481830597
step: 505, loss: 0.2969169318675995
step: 506, loss: 0.21607454121112823
step: 507, loss: 0.05537138134241104
step: 508, loss: 0.14897078275680542
step: 509, loss: 0.08838213235139847
step: 510, loss: 0.09831740707159042
step: 511, loss: 0.07671165466308594
step: 512, loss: 0.040943875908851624
step: 513, loss: 0.12522223591804504
step: 514, loss: 0.21486057341098785
step: 515, loss: 0.16149508953094482
step: 516, loss: 0.21949589252471924
step: 517, loss: 0.09048578143119812
step: 518, loss: 0.06831898540258408
step: 519, loss: 0.04309753328561783
step: 520, loss: 0.10083041340112686
step: 521, loss: 0.09441743046045303
step: 522, loss: 0.13754887878894806
step: 523, loss: 0.16832131147384644
step: 524, loss: 0.11046896874904633
step: 525, loss: 0.0814247727394104
step: 526, loss: 0.15532921254634857
step: 527, loss: 0.08372389525175095
step: 528, loss: 0.12151291221380234
step: 529, loss: 0.10452225059270859
step: 530, loss: 0.11441540718078613
step: 531, loss: 0.06627737730741501
step: 532, loss: 0.1515071839094162
step: 533, loss: 0.31221505999565125
step: 534, loss: 0.138067364692688
step: 535, loss: 0.1329784095287323
step: 536, loss: 0.12689292430877686
step: 537, loss: 0.16735106706619263
step: 538, loss: 0.09835617244243622
step: 539, loss: 0.11483134329319
step: 540, loss: 0.12805208563804626
step: 541, loss: 0.16943641006946564
step: 542, loss: 0.12669190764427185
step: 543, loss: 0.08631342649459839
step: 544, loss: 0.15363460779190063
step: 545, loss: 0.2062772810459137
step: 546, loss: 0.13325348496437073
step: 547, loss: 0.1307401955127716
step: 548, loss: 0.2148369401693344
step: 549, loss: 0.07743927091360092
step: 550, loss: 0.10158202797174454
step: 551, loss: 0.08487920463085175
step: 552, loss: 0.15848134458065033
step: 553, loss: 0.144027441740036
step: 554, loss: 0.15529075264930725
step: 555, loss: 0.07131318747997284
step: 556, loss: 0.12084266543388367
step: 557, loss: 0.15100209414958954
step: 558, loss: 0.11144380271434784
step: 559, loss: 0.14954565465450287
step: 560, loss: 0.031941045075654984
step: 561, loss: 0.0876413881778717
step: 562, loss: 0.07736577093601227
step: 563, loss: 0.09927994757890701
step: 564, loss: 0.08993756026029587
step: 565, loss: 0.09349113702774048
step: 566, loss: 0.06500022858381271
step: 567, loss: 0.1584862321615219
step: 568, loss: 0.10252677649259567
step: 569, loss: 0.12172965705394745
step: 570, loss: 0.10084202140569687
step: 571, loss: 0.11242921650409698
step: 572, loss: 0.16127227246761322
step: 573, loss: 0.0562550351023674
step: 574, loss: 0.25770801305770874
step: 575, loss: 0.05843927338719368
step: 576, loss: 0.24069419503211975
step: 577, loss: 0.11724700778722763
step: 578, loss: 0.1198553815484047
step: 579, loss: 0.06932900100946426
step: 580, loss: 0.10538455843925476
step: 581, loss: 0.2143419235944748
step: 582, loss: 0.17930437624454498
step: 583, loss: 0.06178336963057518
step: 584, loss: 0.2504945993423462
step: 585, loss: 0.1168360710144043
step: 586, loss: 0.012273063883185387
step: 587, loss: 0.09679889678955078
step: 588, loss: 0.12858767807483673
step: 589, loss: 0.17064787447452545
step: 590, loss: 0.10340265184640884
step: 591, loss: 0.05644885450601578
step: 592, loss: 0.07803189754486084
step: 593, loss: 0.024537909775972366
step: 594, loss: 0.08953578770160675
step: 595, loss: 0.09131592512130737
step: 596, loss: 0.21019825339317322
step: 597, loss: 0.09328211843967438
step: 598, loss: 0.08151983469724655
step: 599, loss: 0.22672675549983978
step: 600, loss: 0.08986048400402069
step: 601, loss: 0.09570541232824326
step: 602, loss: 0.07874780148267746
step: 603, loss: 0.1271870881319046
step: 604, loss: 0.054268959909677505
step: 605, loss: 0.07455819100141525
step: 606, loss: 0.19467860460281372
step: 607, loss: 0.08521690219640732
step: 608, loss: 0.09621699154376984
step: 609, loss: 0.29128944873809814
step: 610, loss: 0.23449920117855072
step: 611, loss: 0.3278926908969879
step: 612, loss: 0.19930531084537506
step: 613, loss: 0.28606584668159485
step: 614, loss: 0.21424593031406403
step: 615, loss: 0.215784952044487
step: 616, loss: 0.15508459508419037
step: 617, loss: 0.057998236268758774
step: 618, loss: 0.11187627911567688
step: 619, loss: 0.16818925738334656
step: 620, loss: 0.13142243027687073
step: 621, loss: 0.17741338908672333
step: 622, loss: 0.1333560049533844
step: 623, loss: 0.11708788573741913
step: 624, loss: 0.2859646677970886
step: 625, loss: 0.18185928463935852
step: 626, loss: 0.19378848373889923
step: 627, loss: 0.2557452917098999
step: 628, loss: 0.11965663731098175
step: 629, loss: 0.2061873823404312
step: 630, loss: 0.20471088588237762
step: 631, loss: 0.22025716304779053
step: 632, loss: 0.1604902595281601
step: 633, loss: 0.09908438473939896
step: 634, loss: 0.11812806874513626
step: 635, loss: 0.1472245752811432
step: 636, loss: 0.08529389649629593
step: 637, loss: 0.2497149109840393
step: 638, loss: 0.12479676306247711
step: 639, loss: 0.12434187531471252
step: 640, loss: 0.1414312869310379
step: 641, loss: 0.12107228487730026
step: 642, loss: 0.05493529140949249
step: 643, loss: 0.09483709186315536
step: 644, loss: 0.1352706104516983
step: 645, loss: 0.12203442305326462
step: 646, loss: 0.22022664546966553
step: 647, loss: 0.13081271946430206
step: 648, loss: 0.15334458649158478
step: 649, loss: 0.18985386192798615
step: 650, loss: 0.10571034252643585
step: 651, loss: 0.09980243444442749
step: 652, loss: 0.21378906071186066
step: 653, loss: 0.09325387328863144
step: 654, loss: 0.226312518119812
step: 655, loss: 0.1822860836982727
step: 656, loss: 0.19499541819095612
step: 657, loss: 0.217726930975914
step: 658, loss: 0.13887500762939453
step: 659, loss: 0.11219026893377304
step: 660, loss: 0.14097638428211212
step: 661, loss: 0.12317505478858948
step: 662, loss: 0.1460849493741989
step: 663, loss: 0.1902640461921692
step: 664, loss: 0.1197453960776329
step: 665, loss: 0.06347261369228363
step: 666, loss: 0.10615058243274689
step: 667, loss: 0.1936471313238144
step: 668, loss: 0.10597565770149231
step: 669, loss: 0.06366056203842163
step: 670, loss: 0.06778743118047714
step: 671, loss: 0.1507229059934616
step: 672, loss: 0.06568756699562073
step: 673, loss: 0.15775400400161743
step: 674, loss: 0.17004719376564026
step: 675, loss: 0.05289318040013313
step: 676, loss: 0.08961272984743118
step: 677, loss: 0.05320354551076889
step: 678, loss: 0.09544213116168976
step: 679, loss: 0.18372632563114166
step: 680, loss: 0.07923067361116409
step: 681, loss: 0.09152965247631073
step: 682, loss: 0.15032263100147247
step: 683, loss: 0.2230561077594757
step: 684, loss: 0.1443699300289154
step: 685, loss: 0.09464502334594727
step: 686, loss: 0.1226843073964119
step: 687, loss: 0.2477743923664093
step: 688, loss: 0.15270619094371796
step: 689, loss: 0.15506486594676971
step: 690, loss: 0.1498907506465912
step: 691, loss: 0.014027233235538006
step: 692, loss: 0.06167338043451309
step: 693, loss: 0.18092316389083862
step: 694, loss: 0.05155932903289795
step: 695, loss: 0.06982003897428513
step: 696, loss: 0.06504655629396439
step: 697, loss: 0.18889714777469635
step: 698, loss: 0.0652320608496666
step: 699, loss: 0.06101755052804947
step: 700, loss: 0.193592831492424
step: 701, loss: 0.168085515499115
step: 702, loss: 0.08551499992609024
step: 703, loss: 0.08316720277070999
step: 704, loss: 0.11986489593982697
step: 705, loss: 0.05743434280157089
step: 706, loss: 0.05958795174956322
step: 707, loss: 0.055376194417476654
step: 708, loss: 0.11504210531711578
step: 709, loss: 0.06741699576377869
step: 710, loss: 0.12512940168380737
step: 711, loss: 0.027323978021740913
step: 712, loss: 0.11004075407981873
step: 713, loss: 0.057567697018384933
step: 714, loss: 0.17331098020076752
step: 715, loss: 0.11926484107971191
step: 716, loss: 0.048823870718479156
step: 717, loss: 0.005614008754491806
step: 718, loss: 0.21347492933273315
step: 719, loss: 0.0767589583992958
step: 720, loss: 0.04649709165096283
step: 721, loss: 0.07010602205991745
step: 722, loss: 0.14057330787181854
step: 723, loss: 0.07668029516935349
step: 724, loss: 0.11755198240280151
step: 725, loss: 0.0861358642578125
step: 726, loss: 0.08516452461481094
step: 727, loss: 0.08442609012126923
step: 728, loss: 0.07518191635608673
step: 729, loss: 0.029095249250531197
step: 730, loss: 0.04121054708957672
step: 731, loss: 0.10953851789236069
step: 732, loss: 0.04896406829357147
step: 733, loss: 0.17533813416957855
step: 734, loss: 0.020373865962028503
step: 735, loss: 0.020391983911395073
step: 736, loss: 0.07462210953235626
step: 737, loss: 0.06418165564537048
step: 738, loss: 0.11828665435314178
step: 739, loss: 0.02324238233268261
step: 740, loss: 0.08303224295377731
step: 741, loss: 0.18729940056800842
step: 742, loss: 0.15718884766101837
step: 743, loss: 0.008590917102992535
step: 744, loss: 0.04907042533159256
step: 745, loss: 0.15275615453720093
step: 746, loss: 0.07282087951898575
step: 747, loss: 0.15316423773765564
step: 748, loss: 0.08325336128473282
step: 749, loss: 0.037780746817588806
step: 750, loss: 0.010458989068865776
step: 751, loss: 0.1023208498954773
step: 752, loss: 0.10786178708076477
step: 753, loss: 0.1572185456752777
step: 754, loss: 0.15797105431556702
step: 755, loss: 0.06266405433416367
step: 756, loss: 0.08109612762928009
step: 757, loss: 0.14134985208511353
step: 758, loss: 0.14155380427837372
step: 759, loss: 0.09970862418413162
step: 760, loss: 0.2544390559196472
step: 761, loss: 0.1439613550901413
step: 762, loss: 0.07866677641868591
step: 763, loss: 0.06482794135808945
step: 764, loss: 0.15113578736782074
step: 765, loss: 0.11824310570955276
step: 766, loss: 0.07272306084632874
step: 767, loss: 0.06586072593927383
step: 768, loss: 0.044797949492931366
step: 769, loss: 0.12038339674472809
step: 770, loss: 0.1146727204322815
step: 771, loss: 0.10954359173774719
step: 772, loss: 0.12391002476215363
step: 773, loss: 0.13747885823249817
step: 774, loss: 0.03380759805440903
step: 775, loss: 0.2605656087398529
step: 776, loss: 0.08881973475217819
step: 777, loss: 0.19858096539974213
step: 778, loss: 0.12080949544906616
step: 779, loss: 0.3601859211921692
step: 780, loss: 0.18140845000743866
step: 781, loss: 0.14933693408966064
step: 782, loss: 0.1943945437669754
step: 783, loss: 0.19435900449752808
step: 784, loss: 0.08027619123458862
step: 785, loss: 0.03719571977853775
step: 786, loss: 0.06568499654531479
step: 787, loss: 0.0982411578297615
step: 788, loss: 0.027734480798244476
step: 789, loss: 0.08335180580615997
step: 790, loss: 0.10489954799413681
step: 791, loss: 0.05752422288060188
step: 792, loss: 0.10198690742254257
step: 793, loss: 0.1443711668252945
step: 794, loss: 0.03294363617897034
step: 795, loss: 0.12017276883125305
step: 796, loss: 0.15881185233592987
step: 797, loss: 0.08204802870750427
step: 798, loss: 0.16541005671024323
step: 799, loss: 0.02742166817188263
step: 800, loss: 0.15272945165634155
step: 801, loss: 0.07792259752750397
step: 802, loss: 0.09471450746059418
step: 803, loss: 0.13286952674388885
step: 804, loss: 0.1369134932756424
step: 805, loss: 0.1284540593624115
step: 806, loss: 0.09252943843603134
step: 807, loss: 0.110158272087574
step: 808, loss: 0.12671975791454315
step: 809, loss: 0.11650367826223373
step: 810, loss: 0.08684376627206802
step: 811, loss: 0.14436018466949463
step: 812, loss: 0.09060849994421005
step: 813, loss: 0.08058994263410568
step: 814, loss: 0.12592953443527222
step: 815, loss: 0.05857563018798828
step: 816, loss: 0.05735272541642189
step: 817, loss: 0.12099548429250717
step: 818, loss: 0.17807041108608246
step: 819, loss: 0.1259356290102005
step: 820, loss: 0.15766799449920654
step: 821, loss: 0.1287120282649994
step: 822, loss: 0.09596657752990723
step: 823, loss: 0.06386103481054306
step: 824, loss: 0.13317686319351196
step: 825, loss: 0.053337886929512024
step: 826, loss: 0.03827653080224991
step: 827, loss: 0.07687024027109146
step: 828, loss: 0.0862404853105545
step: 829, loss: 0.1851954311132431
step: 830, loss: 0.11990664154291153
step: 831, loss: 0.15012869238853455
step: 832, loss: 0.17867511510849
step: 833, loss: 0.047390297055244446
step: 834, loss: 0.14881013333797455
step: 835, loss: 0.0880199447274208
step: 836, loss: 0.15351039171218872
step: 837, loss: 0.1521507054567337
step: 838, loss: 0.13582178950309753
step: 839, loss: 0.10996954143047333
step: 840, loss: 0.10370353609323502
step: 841, loss: 0.1359131634235382
step: 842, loss: 0.20302562415599823
step: 843, loss: 0.08601019531488419
step: 844, loss: 0.17847654223442078
step: 845, loss: 0.17456969618797302
step: 846, loss: 0.1564496010541916
step: 847, loss: 0.19514818489551544
step: 848, loss: 0.10643387585878372
step: 849, loss: 0.09410517662763596
step: 850, loss: 0.05451241880655289
step: 851, loss: 0.1505453735589981
step: 852, loss: 0.03802734240889549
step: 853, loss: 0.13401538133621216
step: 854, loss: 0.0817735567688942
step: 855, loss: 0.08509755879640579
step: 856, loss: 0.14403584599494934
step: 857, loss: 0.1450503021478653
step: 858, loss: 0.1965240091085434
step: 859, loss: 0.13016285002231598
step: 860, loss: 0.11173129081726074
step: 861, loss: 0.02679010108113289
step: 862, loss: 0.12074282020330429
step: 863, loss: 0.14207953214645386
step: 864, loss: 0.007810743059962988
step: 865, loss: 0.07338998466730118
step: 866, loss: 0.2056618183851242
step: 867, loss: 0.14233851432800293
step: 868, loss: 0.1443445235490799
step: 869, loss: 0.04772167280316353
step: 870, loss: 0.13339978456497192
step: 871, loss: 0.10849454253911972
step: 872, loss: 0.17904765903949738
step: 873, loss: 0.09144708514213562
step: 874, loss: 0.08070172369480133
step: 875, loss: 0.0899907797574997
step: 876, loss: 0.10832071304321289
step: 877, loss: 0.06458237022161484
step: 878, loss: 0.057768963277339935
step: 879, loss: 0.10034067183732986
step: 880, loss: 0.07185887545347214
step: 881, loss: 0.06489492207765579
step: 882, loss: 0.09570774435997009
step: 883, loss: 0.09711271524429321
step: 884, loss: 0.2235858142375946
step: 885, loss: 0.18479076027870178
step: 886, loss: 0.055146921426057816
step: 887, loss: 0.10132050514221191
step: 888, loss: 0.0639919564127922
step: 889, loss: 0.09018260985612869
step: 890, loss: 0.15384311974048615
step: 891, loss: 0.09825113415718079
step: 892, loss: 0.11064128577709198
step: 893, loss: 0.14288192987442017
step: 894, loss: 0.10209611803293228
step: 895, loss: 0.14841388165950775
step: 896, loss: 0.10339152067899704
step: 897, loss: 0.1108541190624237
step: 898, loss: 0.0014824032550677657
step: 899, loss: 0.11980778723955154
step: 900, loss: 0.07033353298902512
step: 901, loss: 0.05929582938551903
step: 902, loss: 0.031038478016853333
step: 903, loss: 0.03011569008231163
step: 904, loss: 0.0554962158203125
step: 905, loss: 0.08637069910764694
step: 906, loss: 0.13339199125766754
step: 907, loss: 0.1369882971048355
step: 908, loss: 0.17041170597076416
step: 909, loss: 0.048460692167282104
step: 910, loss: 0.08887197822332382
step: 911, loss: 0.056394997984170914
step: 912, loss: 0.08646076917648315
step: 913, loss: 0.10281500965356827
step: 914, loss: 0.15307989716529846
step: 915, loss: 0.08784505724906921
step: 916, loss: 0.023734290152788162
step: 917, loss: 0.10412850230932236
step: 918, loss: 0.09284856915473938
step: 919, loss: 0.11947467923164368
step: 920, loss: 0.12348251044750214
step: 921, loss: 0.0547427274286747
step: 922, loss: 0.12361762672662735
step: 923, loss: 0.07074373960494995
step: 924, loss: 0.2464812844991684
step: 925, loss: 0.07479152083396912
step: 926, loss: 0.08837995678186417
step: 927, loss: 0.10457529127597809
step: 928, loss: 0.09444448351860046
step: 929, loss: 0.11491291224956512
step: 930, loss: 0.09346631914377213
step: 931, loss: 0.12643466889858246
step: 932, loss: 0.09152953326702118
step: 933, loss: 0.07903118431568146
step: 934, loss: 0.0477529801428318
step: 935, loss: 0.16141898930072784
step: 936, loss: 0.1497427374124527
step: 937, loss: 0.05772200599312782
step: 938, loss: 0.12438999116420746
step: 939, loss: 0.03731488063931465
step: 940, loss: 0.08299240469932556
step: 941, loss: 0.06722630560398102
step: 942, loss: 0.07237312942743301
step: 943, loss: 0.08925813436508179
step: 944, loss: 0.11264098435640335
step: 945, loss: 0.13151699304580688
step: 946, loss: 0.07450031489133835
step: 947, loss: 0.05273786559700966
step: 948, loss: 0.05253396928310394
step: 949, loss: 0.09941180050373077
step: 950, loss: 0.06573928892612457
step: 951, loss: 0.007391724735498428
step: 952, loss: 0.26516592502593994
step: 953, loss: 0.08018676936626434
step: 954, loss: 0.0499221570789814
step: 955, loss: 0.08223932236433029
step: 956, loss: 0.17689137160778046
step: 957, loss: 0.04613412544131279
step: 958, loss: 0.04532134532928467
step: 959, loss: 0.09242846816778183
step: 960, loss: 0.15610773861408234
step: 961, loss: 0.08995244652032852
step: 962, loss: 0.05843842402100563
step: 963, loss: 0.1475546509027481
step: 964, loss: 0.05259224399924278
step: 965, loss: 0.14674437046051025
step: 966, loss: 0.15605658292770386
step: 967, loss: 0.20018817484378815
step: 968, loss: 0.045207999646663666
step: 969, loss: 0.06651455163955688
step: 970, loss: 0.09421712160110474
step: 971, loss: 0.00886830035597086
step: 972, loss: 0.13109645247459412
step: 973, loss: 0.06044892221689224
step: 974, loss: 0.11004571616649628
step: 975, loss: 0.04571382701396942
step: 976, loss: 0.07034710794687271
step: 977, loss: 0.1424732655286789
step: 978, loss: 0.02893335558474064
step: 979, loss: 0.013228846713900566
step: 980, loss: 0.13210470974445343
step: 981, loss: 0.05099308118224144
step: 982, loss: 0.10672783851623535
step: 983, loss: 0.042627133429050446
step: 984, loss: 0.047825541347265244
step: 985, loss: 0.13334183394908905
step: 986, loss: 0.19182251393795013
step: 987, loss: 0.06843804568052292
step: 988, loss: 0.08572759479284286
step: 989, loss: 0.2002602368593216
step: 990, loss: 0.034929171204566956
step: 991, loss: 0.05200553312897682
step: 992, loss: 0.05270041525363922
step: 993, loss: 0.06224983558058739
step: 994, loss: 0.06458161026239395
step: 995, loss: 0.18477188050746918
step: 996, loss: 0.18191154301166534
step: 997, loss: 0.022210029885172844
step: 998, loss: 0.12281585484743118
step: 999, loss: 0.0846375823020935
step: 1000, loss: 0.03541480377316475
Epoch: 4
Accuracy on test data: 0.12599575146043548
